\chapter{30/10/2019}
\subsection{Underflow}
It can happen that multiplying for many number, some of them tend to 0 which returns 0. To solve this probblem, instead of considering $\max{p(X)}$ we consider $\log{max{P(x)}}$ which can be converted to sum. 
\subsection{Exact inference}
Let's see how to deal with inference when the network structure is not as nice as the one we have right now. 
\subsection{Approximate inference}
Another possible solution is based on sampling: suppose to have $p(x)$ and we want to compute $p(Y=y\vert E=e)$ and we can't expliciely compute this throw message passing. What we do is to count the number of samples that satisfy the condition, that is out of the sample that have $e$, they have also $y$. The problem is that sampling from a distribution is difficult. We need to build to a process that allows us to do this and this is: Markov chain Monte Carlo. 
\chapter{Learning in Graphical Models}
We'll see how to apply what we have seen for parameters learning to Bayesian networks.\newline
In a prob graphical model, when doing param estimation, that means that the structure of the model is already there. To estimate the parameters we have a dataset and each $x$ in the dataset is a configuration for the features. The simplest solution is to max the likelihood of the parameters given the data.\newline
BN can be used not only to reppresent configruation of variables, but also to reason about what we want to learn. Suppose we have a simple graph with tale to tale connections. We also ha ve a datset of examples, which can be specified by repeting the network $N$ times, where each copy refers to the network for a certain example. The parameters of the network are the ones in blue, and they are connected to the nodes that depend on the parameters. Now to compute the likelihood:
\begin{center}
	$\displaystyle p(D\vert \theta)$
\end{center}
The fact that the parameters are given, it means that the data are independent from one an another and they are iid independent and bbohh. \newline
Let's now try to see how to get the parameters. If we want to get the probability of boh, we don't have to look at all $\theta$ but only at that specific $\theta$. Moreover in slide 4? we are computing the probability on all nodes, but if each node dependes only on its table (parent?), then we can multiply only between the nodes for which we maximize the parameters????????
Let's say we have this table sui fogli. allora possiamo massimizzare per ogni colonna quindi
\begin{center}
	$\displaystyle argmax_{\theta_{T\vert T}, \theta_{F\vert T}}=\Pi_{i=x_2(i)=T}P(x_2(i)\vert x_2(i)=T, \theta_{T\vert T}, \theta_{F\vert T}$
\end{center}
E poi l'altra per l'altra colonna... \newline
Il numero di colonne è il prodotto di tutti i possibili padri per il numero di possibili valori che assume. Vedi quaderno 

\chapter{31-10-2019}
\chapter{Laboratorio}
\begin{center}
	$\displaystyle P(alarm\vert alarm=on)=1$\\
	$P(burglar\vert A=on)=$\\
	$P(E\vert A=on)=$\\
	$P(C\vert A=on)=$
\end{center}
\begin{center}
	$\displaystyle P(B=N,E=Y\vert C=Y=$
\end{center}
To compute this last thing we can use the probbability of evidence since it can be computed as 
\begin{center}
	$\displaystyle \frac{P(B=N, E=Y,C=Y)}{P(C=Y)}$
\end{center}
$P(C=Y)$ can be obtained by putting the evidence on the graph: $P(C=Y)=0.3709$. The same for the numerator:
\begin{center}
	$\displaystyle P(B=N, E=Y,C=Y)=0.0427$
\end{center}
These are computed actually as the sum over the product of joint prob:
\begin{center}
	$\displaystyle P(B=N, E=Y,C=Y)=\Sum_A P(C\vert A)P(A\vert B E)P(E)P(B)$
\end{center}
Then plug values for the non missing variables:
\begin{center}
	$\displaystyle P(B=N, E=Y,C=Y)=\Sum_A P(C=Y\vert A)P(A\vert B=N E=Y)P(E=Y)P(B=N)$
\end{center}

\chapter{06/11/2019}
\chapter{Evaluation}
We need to define a performance measure or more than one. They need to be quantitative.\newline
In principle when training a model we'd like to know its performance on the entire domain where it's going to be applied, but this is not possibile since we'd need labelled example and this is not always possible.\newline
The biggest use of evaluation is to know the performance of the predictor when applied into the real world. We can't evaluate this on the training set because of overfitting.\newline
When doing training we need to take some decisions that needs to be fixed at the beginning, for example the number of scoring function to divide the decision trees, or the number of neighbours. This is called hyper parameters tuning, that is parameters that are not learned during training but need to be fixed at the beginning. Also this can be machine learned, that is trying different parameters and seeing which one is the best.\newline
When we talked about parameters estimation, we talked about maximum likelihood and ecc. This is the training loss function and it's what we want to maximise. Instead we talk about training loss we intend what we want to minimise. 
\section{Binary classification}
We use a confusion matrix, that is a matrix that reports the true labels and the predictive labels so to have positive-negative, positive-positive ecc. \newline
Each cell will have the number of prediction that were predicted in a way with respect to their real value.\newline
This matrix is useful because ti gives a picture of what the predictor does. For example consider the followin table:
\begin{center}
	\begin{tabular}{ccc}
	& Pos&neg\\
	pos& 0 &10\\
	neg& 0 &90
	\end{tabular}
\end{center}
There some metrics we can compute:
\begin{itemize}
	\item Accuracy: it says when how many were predicted correctly. It's a scoring function, so the higher the better. This cannot be used for training, its also problematic as an evaluation metric as some datasets are too umbalanced as the table before. The accuracy of the previous was 0.9, but this has learned only to predict negatives. If we dind't look at the confusion matrix we wouldn't have noticed. 
	\item Precision: is how accurate the model is when predicting positive: among the example that were predictive positive, how many were right? 
	\item Recall: the complement of precision: how many positive example we got with respective to the true positive. This two measures are contradictory: if we want to be really precise we'll predict positive less frequently and so recall will be lower.
\end{itemize}
There are then aggregate measures such as F-measure that combina precision and recall. This is also parametrised by $\beta$, which is usually 1. When $\beta=1$ then it's called harmonic mean. What happens when precision grows or recall grows?\newline
We can also define also the full curve of the model. By playing with a threshold we can go towards precision or recall. This is something that is application dependent, in principle. In the graph there is discontinuity when recall is 0 since also precision is 0. This allows us to compare algorithms when we know what we want to achieve and we are more interested in recall rather than precision and viceversa. We can compiute the area below the curve and the bigger the area, the better the algorithm overall quality, but usually it's best to compare stuff.
\section{Multiclass}
Binary matrix can be extent to multi-class. It's just binary but working per class.\newline
Even the metrics are computed per class. There is then a global metric that's called multiclass accuracy that is the sum of true positives divided by the same of all elements in the matrix.\newline
Another important coefficient is the Pearson correlation coefficient that has some properties. This could no be computed exactly in term of coefficient but need to take some sample and get sample variance and covariance.
\subsection{Hold-out}
It means taking dataset, splitting it in a subset for training, one for validation (in order to take the best hyperparameters through the metrics seen before) and one for testing. Once evaluated we have hte best choice of everything and we can retrain the model and then revaluate. \newline
This method is used when we have a big test set. In other cases we have k-fold cross valudation. In this case we don't trust the values and we want to have some kind of average. We take the dataset, split it in subgroups called folds and itereate from one to k, we take one fold for validation and all the rest is used for training. Then a score is computed with respect to all the rest and ecc. At the end of the procedure there are $k$ scores that will be averaged. When the dataset is really small we do stratified k-fold cross validation so that the statistics of the dataset are replied in the folds.\newline
Other than the average, we can compute the variance that by the law of the large number as:
\begin{center}
	$\displaystyle Var[\widehat{S}]=Var\left[\frac{S_1+...+S_k}{k}\right]=\frac{1}{k^2}\Sum_{j=1}^kVar[S_j]$
\end{center}
This is true if the sets are independent, and this is not entrelly true since the validations sets (or training) have some values in common. For this reason we cannot compute it preciselly, but we can estimate it:
\begin{center}
	$\displaystyle Var[S_j]=Var[S_h]\approx\frac{1}{k-1}\Sum_{i=1}^k(S_i-\widehat{S})^2$
\end{center}
Which give what's in slide. \newline
Through this we can have a variance of the crosso validate parameters.
\section{Comparing different learning algorithms}
For sure they can be compared through cross validation: compare one, then compare another and then choose. But if we want to propose a new method we need to be sure that the differences seen are significant. The most precise is to use hypothesis testing that is statistical stuff.\newline
What we want to show is that the performance of two algorithms are statistical significant, that is the difference in performance of the two is statistical significant. \newline
Usually we have a null hypothesis and we want to provide prof to reject this hypothesis, for example showing that the results of two algorithms are different. \newline
tail probability: when talking of statistical test we assume some belled model and this is the probability of the extremes. \newline
p-value: the lower the more significant the test.\newline
Type 1 error: reject the hypothesis when it's true. This is the most grave since it says that we cannot exclude that two algorithms are the same but we cannot even say they are.\newline
To decide over two algorithms we'll use a standardised mean:
\begin{center}
	$\displaystyle T=\frac{\widehat{X}-\mu_0}{}$
\end{center}
Where $\widehat{X}$ is and $\mu_0$ is the mean of the null distribution, while var is the variance we computed before. \newline
If we assume that the samples come from a normal, then it's a t-distribution which is a k parametrized distribution with k-1 degree of freedom with k as number of folds?\newline
Una o due slide dopo, allora la null hypothesis is that the mean is 0. Which substituting the new value computed $\widehat{\delta}$ and $\mu_0=0$ si ottiene quello nella slide dopo. A questo punto dobbiamo decidere in particolare qual è la probabilià di avere un type 1 error. This is called significance level $\alpha$ and for example we say we accept to have a $5\%$ type 1 error. The less this value, the less the probability to have a type 1 error.\newline
This test in particular is called pair test because when it's computed, the two sample estimate, come from the same fold, so we could compare the statistic on a certain fold. Moreoever it's also called two-tailed test because we cannot exclude at prior neither of the outcomes, that is if ono algo is better than the other and viceversa. 
\chapter{07/11/2019}
\chapter{Discriminative vs generative}
If you go with generative you need to model all the $x$s which can be pretty difficult. For example if we wanted to model a whole website modelling all the links ecc with be really difficult, expecially if for example we'd wanted to know the argument of the site. For doing this we can use a discriminative model where we only care about the boundaries. This is also a downside of this approach: if the inputs are really different then boundaries cannot be extracted. For this reason a discriminative model cannot 6:20 and it's not used to generate new samples. 
\section{Linear discriminative model}
It uses a linear function such as:
\begin{center}
	$\displaystyle f(x)$
\end{center}
Where $W_0$ is called the bias or threshold. \newline
This is the simplest function, which means also not the best one, but in most of the cases is perfect.\newline
For classification we take the sign of the function: is negative is one thing, if positive the other, otherwise if 0, then is the decision boundary, that is the set of points for which the discriminant function is 0. \newline
The decision function is in reality a decision hyperplane. 
\chapter{Biological motivation}
The linear classifier, perceptron, take source from the brain which is a colleciton of neurons. These are made out of a soma circondato da dentriti e il più lungo di questi, detto axon, che si collega ad altri neuroni tramite le sinpasi. \newline
Electrochemical eractions allows to propagate the signal in the brain. \newline
A perceptron as a set of inputs, for example the features, which get maximised by weights and then summed and finally through an activation function, it mimics the activation of the neuron so it's a threshold. This is the simpler formulation for a linear binary classifier.\newline
A linear classifier (perceptron) can linearly separate classes that are linearly separable, for example boolean function. In this case the standard functions such as AND and OR are linearly separable: two feature can be either 1 or 0. In the OR the result can be either positive or negative and the result is then easily separable by a line. The same can be said for the AND. The same cn be said for the not which is just a value and for that a threshold again. Let's consider the XOR, then dividing by an hyperplane is not possibile and for this, this is not a linear separable function \ins{foto}. To solve the XOR we can use a multy level perceptron. We know that it corresponds to $A\wedge not B)\vee (not A\wedge B)$ so this can be computed separately first and then solved. The structure is the one in \ins{foto}. We know that in principle every boolean formula can be learned by a two layer perceptron since all the formula can be expressed in conjunctive or disjunctive normal formula. Why then do we need multi strata deep networks? Because the conjunctive, disjunctive normal formulas are exponentially in term of number of neurones. \newline
In the picture of the slide perceptron there is a bias which is like an additional input which is always zero and one more wight. For this we can write $f(x)$ as augmented vectors like in slide. From now one all the formula will include bias and weight without specifying.\newline
The weights are learned from the data. We decide an error function, what we call a training loss which usually is the sum over the training exame of the loss encorred in predicting something while the actual value was something else. There is a problem of overfitting since I could just memorise training example. This acutally is not a problem with linear classifiers, but with more difficutl system this is not sufficient. \newline
Now suppose what king of loss function to use, then we can do error minimisation by changing the weights. What we can always do is gradient descent. Let's suppose we have a singular parameter and the error function over the parameter is the one in \ins{foto}. To minimise it, we start from x=0, we compute the gredient and then go opposite of the gradient (otherwise we'd maximise). Then we compute the gradient again and so on until we reach the minimum. The learning rate $\eta$ tells us how much we need to move each time. With gradient descent we reach a local minimum. \newline
In perceptron we use a confident margin: we do not take the sign of $f(x)$ but we multiply $f(x)$ with y, so the bigger the result the bigger the error.\newline 
This is called batch training because every step we compute the error on the entire training set. There is also a version where we take on example each time and compute the error. If it was correctly classified then ok, otherwise we compute the gradient. This is called stochastic perceptron and is usually pretty fast. In some cases we can use only stochastic since the dataset would be too big, moreover it allows to avoid local minima since each time the gradient function is a little different.\newline
Con il gradiente praticamente si gira pian piano finchè tutti gli errori non scompaiono.\newline
\section{Percpetron regression}
With linear predictors we can also to regression other the classification as done now. \newline
Let's say we have $n$ exmaple, and that each example has $\vert D\vert$ feature. Let's take each example and put them into a matrix as columns. The desired output is $n$ dimentional vector of real values. Given the function:
\begin{center}
	$\displaystyle $
\end{center}
The perfect regression would be:
\begin{center}
	$\displaystyle \forall i, y_i=w^Tx_i$
\end{center}
But since we have matrix:
\begin{center}
	$\displaystyle Xw=y$
\end{center}
Which linear system can be solved as:
\begin{center}
	$\displaystyle w=X^{-1}y$
\end{center}
But this doesn't work since $X$ is not revertible in many cases. If we are lucky we are going to have more example than features, otherwise just don't. This means that there is no exact solution, so we cannot do this, but rather we should forget about having a right solution, but trying to minimise the error, which is the square difference of $f(x)$ and $y$:
\begin{center}
	$\displaystyle (xw-y)^T(Xw-y)$
\end{center}
\chapter{20/11/2019}
\chapter{Support vector regression}
$\varepsilon$ and $\epsilon$ are swapped\newline
The constraint we want to specify are:
\[\forall i
\begin{cases}
	y_i-f(x_i)\leq \varepsilon\\
	f(x_i)-y_i\leq \varepsilon
\end{cases}
\]
And then we want to minimise the normal weight subjects to the constraints:
\begin{center}
	$\displaystyle min\frac{\vert\vert w\vert\vert^2}{2} s.t. constraints$
\end{center}
We assume that all training instances satisfy the constaints and are therefor within $\varepsilon$: their prediction is out of outmost $\varepsilon$.\newline
As in classification, we can relax the constraints that all constraints are satisfied by adding slack variables. So if in classification we had:
\begin{center}
	$\displaystyle min\frac{\vert\vert w \vert\vert^2}{2} \forall i y_i-f(x_i)\geq 1$
\end{center}
Which adding a slack variable becomes:
\begin{center}
	$\displaystyle min\frac{\vert\vert w \vert\vert^2}{2} +C\Sum_i\varepsilon_i \forall i y_i-f(x_i)\geq 1-\varepsilon_i$
\end{center}
So the regression version becomes:
\[\forall i
\begin{cases}
	y_i-f(x_i)\leq \varepsilon+\epsilon_i\\
	f(x_i)-y_i\leq \varepsilon+\epsilon^*_i
\end{cases}
\]
With $\epsilon_i, \epsilon^*_i\geq0$. So we need two different slacks. From these we add to the minimise function:
\begin{center}
	$\displaystyle min\frac{\vert\vert w\vert\vert^2}{2}+C\Sum_i\epsilon_i\epsilon_i^* s.t. constraints$
\end{center}
In this version we do not penalize all example that are outside the queue?\newline
As for classification we can take the last version and use something to get a Langrangian:
\begin{center}
	$\displaystyle \mathcal{L}?\frac{\vert\vertw\vert\vert^2}{2}+C\Sum_i(\epsilon_i+\epsilon?*_i-Sum_i$
\end{center}
Meno la somma di qualcosa come $g_i(x)\geq 0$ che diventa quindi:
\begin{center}
	$\displaystyle \mathcal{L}?\frac{\vert\vertw\vert\vert^2}{2}+C\Sum_i(\epsilon_i+\epsilon?*_i)-Sum_i\alpha_i(\varepsilon+\epsilon_i-y_i+w^T\phi(x_i)+w_0)$
\end{center}
per il primo constraint, e tutto insieme:
\begin{center}
	$\displaystyle \mathcal{L}?\frac{\vert\vertw\vert\vert^2}{2}+C\Sum_i(\epsilon_i+\epsilon?*_i-Sum_i\alpha_i(\varepsilon+\epsilon_i-y_i+w^T\phi(x_i)+w_0)-\Sum_i\alpha_i^*(\varepsilon+\epsilon_i^*-y_i+w^T\phi(x_i)+w_0)-\Sum_i\beta_i\epsilon_i-\Sum_i\beta_i^*\epsilon^*_i$
\end{center}
Da cui il gradiente diventa:
\begin{center}
	$\displaystyle \gradiente_w\mathcal{L}=\frac{2w}{2}-\Sum_i\alpha_i\phi(x_i)+\Sum_i\alpha_i^*\phi(x_i)=0$\\
	$\displaystyle w=\Sum_i(\alpha^*_i-\alpha_i)\phi(x_i)$
\end{center}
Deriviamo ora con rispetto di $w_0$:
\begin{center}
	$\displaystyle \frac{\delta\mathcal{L}}{\delta w_0}=-\Sum_i\alpha_i+\Sum_i\alpha_i^*=0$\\
	$\displaystyle \Sum_i(\alpha_i^*-\alpha_i)=0$
\end{center}
Infine deriviamo anche rispetto a $\epsilon_i$ e per $\epsilon_i^*$ :
\begin{center}
	$\displaystyle \frac{\delta\mathcal{L}}{\delta \epsilon_i}=C-\alpha_i-\beta_i=0$\\
	$\displaystyle \frac{\delta\mathcal{L}}{\delta \epsilon_i^*}=C-\alpha_i^*-\beta_i^*=0$\\
\end{center}
Ora se le rimettiamo insieme?? otteniamo:
\begin{center}
	$\displaystyle \frac{1}{2}\Sum_i(\alpha^*_i-\alpha_i)\phi(x_i))^T(\Sum_j(\alpha_j^*\alpha_j)\phi(x_j))-\Sum_i\alpha_i(\Sum_j(\alpha_j^*-\alpha_j)\phi(x_j))^T\phi(x_i)+\Sum_i\alpha_i^*(\Sum_j(\alpha_j^*-\alpha_j)\phi(x_j))^T\phi(x_i)$
\end{center}
Where the first parentesis is $w^T$ and the second is $w$. Now this becomes:
\begin{center}
	$\displaystyle =-\frac{1}{2}\Sum_i\Sum_j(\alpha^*_i-\alpha_i)(\alpha_j^*-\alpha_j)\phi(x_i)^T\phi(x_j)$
\end{center}
ins{1} \newline
The result is the one in slide and the function becomes:
\begin{center}
	$\displaystyle f(x)=\Sum_i(\alpha_i^*-\alpha_i)\phi(x)^T\phi(x)+w_0$
\end{center}
The solution is coming from the dot product between example e chi cazzo ci sta dietro.\newline
The KKT conditions says that at the point where it's best to maximise or minimise, the sum componente of the langrangian become zero. 
\chapter{Kernel machines}
In order to take svm and apply it to problems that are not linearly separable, we can map data to higher dimentional spaces as seen. The problem with this is that can end up beign very high dimensional: there could be cases where the number of spaces would end up being infinite. Moreover it's exepensive. The idea is to look at the dual formulation of the problem, for both regression and classification, and look for where x appears in the dot product: we can apply a kernel trick and replacing the dot product with a function that computes the same value but using only $x$ and $x'$ instead of $\phi(x)$ and $\phi(x')$, so just to work on the examples instead of the features space. \newline
Let's suppose we are interested in working with feature map polynomial. Let's start with homogeneous polynomial, so we want to map: \ins{2}. But we could instead use a kernel that is:
\begin{center}
	$\displaystyle k(x,x')=(X^Tx')^d$
\end{center}
So it depends only on the complexity of the example space. This is called a polynomial kernel. From the formulation in slide: we want to try and separate in order to find the two vectors: \ins{3}. 
In case of inhomogeneous, so there are not just products of features of degree $d$, but also products of features of degree up to $d$. What we do is almost the same as before:
\begin{center}
	$\displaystyle k(x, x')=(1+x^Tx')^d$
\end{center}
Just by adding 1 to the homogeneous polynomial kernel, we obtain an inhomogeneous. \newline
A kernel is a function in input space between pair of entities to the real, which corresponds to a dot product in features space. If this is the case, then $k$ is a valid kernel and for this reason it's usable. We don't have to show that they are the same, for example if the features wouls become infinte, but we just need to know and prove that there is. \newline
Kernels can be also thought as similarity. The idea is that it allows the idea of dot product to arbitrary spaces. \newline
Gram matrix: is the kernel matrix between pairs of examples in a set of examples. Let's say we have a set of examples: $\{x_1, ..., x_m\}$ then we could write:
\begin{center}
	$\displaystyle K_{ij}=k(x_i, x_j)\forall i, j$
\end{center}
How can we say if the kernel is valid or not? We said that is valid if it corresponds to a dot product, but there are also some properties that help us out. A valid kernel is a positive semi-definite matrix. This is true if the following inhequality holds: 
\begin{center}
	$\displaystyle \Sum_{i,j=1}^m c_ic_jK_{ij}\geq 0 \forall c\in\mathbb{R}^m$
\end{center}
Another possibility is to test if all the highen values are non negatives. \newline
Moreover another condition is that slide. \newline
There is a problem though: the kernel is a function, not a kernel, and this needs to be positive on all the space. A kernel then is positive definite if there is a gram matrix for any example.

\chapter{21/11/2019}
\section{Basic kernels}
\subsection{Gaussian kernel}
The inputs are vectors, but there are also kernels that could be applied to objects that are not vectors and those kernels measure their similarity. \newline
The new kernel that we didn't see yet is called gaussian kernel and it's like putting a gaussian between the two examples. The gaussian is not normalize: we don't care about the distribution so it's fine. \newline
\ins{1} that is the input space.\newline
The gaussian kernel it's universal, that is we can uniformaly approximate all example with this kernel. The problem is that the precision of this function depends on:
\begin{itemize}
	\item The choice of $\sigma$ that has to be chosen through validation since it's a beta parameter. 
	\item The number of examples: more it's better. 
\end{itemize}
\section{Structure data}
Not vectors but discrete structure like a sequence or a website. \newline
In this case the idea behind kernel, that is similarity, is very useful. This allows to run a lot of also on top pf our code, like classification bla bla bla, provided that we have an appropriate similarity function between element. What we do is to detach the learning also from the processing of the data, the part that is data specific. At this point we can define kernel over discrete objects and we do this in a compositional way by defining kernels on specific type of structure, like tree, graphs ecc. \newline
The simplest kernel is called match and simply is 1 if the two objects are the same, 0 otherwise. This does not allow to learn anything, but it's just memorisation. But this can be used to strip down a structure and return a value based on the similarity. \newline
Consider the case where we have two strings of different size. This specific case is called spectrum kernel because it measure the frequency of subdomain, in this case substrings. The spectrum kernel computes the similarity by emulating a dot product in the frequency domain, that is how many time a subsequence appear in each string and then dot product between them. \newline
\subsection{Kernel combination}
We combine kernel by applying rules in order to maintain validity of the kernel. 
\subsubsection{Kernel sum}
The sum of two kernels is its-self a kernel and corresponds to the concatenation of their respective feature spaces. The passages in slide are given by the fact that each kernel is valid. To obtain the last formula from the middle one it's like before: move x' to one side, x to the other and bam.  \newline
\subsubsection{Kernel product}
\subsubsection{Linear combination}
Providing a non negative constant. This allows to have a linear combinations of weighted (different) kernels. Weights can be choosen in various ways, for example in kernel learning, that is learning all kernels simultaneously. 
\subsubsection{Kernel decomposition}
In some cases, for example web sites, a linear combination of kernels is not enough. We'd need decomposition kernel, also called convolutional kernels. This breaks a structured object in to pieces.\newline
For example if $x$ is a string we could define a decomposition relation that decompose $x$ in a fix number of substrings $d$ so that their concatenation gives $x$. Obviosly there is not only a set of decomposition, but there a whole bunch of set of decompositions that recover the substring.\newline
This is useful because sometimes it's difficult to combine in a structure object stuff, so we need to find a way to combine objects in a reasonable way.\newline
A convolution kernel is based on a decomposition relationship $R$, so we take $R(x)$ and $R(x')$ two sets. Then we scan all set in $R(x)$ (the first sum), and from $R(x')$, and then we compare. Thje two decomposition have the same number of pieces $d$, so we compare all possible combination of pieces through a kernel, and take the product (which is like a convulation). 
\chapter{Kernel on structures}
\chapter{27-11-2019}
\section{Mismatch string kernel}
If you consider a k-gram, we expand it to its neighbourhood, and two k-gram match if they are in their respective neighbourhoods. In practice it's like a k-gram counts for all k-grams in the neighbourhood.\newline
Let's consider the example in slide: 3-gram with one mismatch, that means for example if we take ABA we can have BBA, AAA, ABB. In this way what happens is that the feature map get more dense since each entry do not partecipate only but also in neighboorhood. This implies for example that ABA and AAA matches since they are one mismatch away. Also ABA and AAB count as a match since they are in each others neighboorhoods. Moreover we don't have to explicity compute this feature map, but there is a strcture, called mismatch tree which is similar to the suffix tree, and create a structure over the two strings that allows to compute the kernel without having to generate the feature maps. 
\section{Kernel on trees}
One can define a kernel on trees, and like kernel on structure where we try to find similarity between complex objects by finding similarity over pieces of the objects, so for trees we use subset trees, that is a subtree having either all or no children of a node (the idea id for example not to break grammar rules). The number of subset tree is exponential with the number of nodes. Once we have all the subset trees we can compare the number of matches and say things. \newline
\begin{center}
	$\displaystyle k(t, t')=\Sum_{i=1}^M\phi_i(t)\phi_i(t')=\Sum_{n_i\in t}\Sum_{n_j\in t}C(n_i, n'_j)$
\end{center}
Given two trees $t, t'$, if you have an explicit mapping $\phi(t)$ (consider in the vector all subset trees that can possibile come from the tree, which is not infinite, but close to it), then it's possible to write it as the sum over all the possible times $\phi_i(t)$ a subset tree appear in the mapping.\newline
The way to compute this fast without computing $\phi$, we can sum over all the nodes in $t, t'$, and count $C$ the number of common subtrees rooted at $n$. We need a way to efficiently compute $C$ though. The idea is that, if a subset tree does not match, then it's useless to try and match also a larger substructure. So we define a recursive procedure where the base step is to check what's called node matching, that is two nodes match if they have the same label, same number of children, the order of children is fixed (the grammar is sequential) and so each child has the same label of the corresponding child in the other tree. In the example in slide, the node NP-N-John matches NP-N-Emy, but mind that the subset trees do not match since the leaves are different. If two nodes match, this is a precondition to go on comparing, if they don't match then we can stop. The rest of the procedure goes as follow: if $n_i$, $n_j'$ don't match, then there is no common subset tree. If they match there is at least one common subset tree that generated the match. If they match and they are both pre-terminal, that is they have terminal children, that is rules that create words, then we are done since that means that their children do not go on with the tree, so $C(n_i,n_j')=1$. Otherwise if they match, but are not pre-terminals, we have to combine the match we have in that position with the matches from the subtree that start from there, so the recursive step works like this: the $C(n_i, n_j')$ is the product over all the number of children (they both have the same number of children since they match) of 1 plus the $C$ over the $h$th child of both $n_i n_j'$. 1+ since there is at least one match (the first one between two nodes), but in reality this gets a boost for every match. \newline
For the best performance is best to start from the bottom, compute $C$ for all the nodes and then use dynamic programming to compute the rest. Consider the example in slide 13: \ins{1}. \newline
\subsection{Dominant diagonal}
There is a problem with kernel on structure: if we compare objects of different size, we don't get good result, but we could normalize. Moreover though these means that the larger the structure, the more it is similar to itself since we the larger subtrees obviously match, and the bigger the gap with the similarity to the other object. To avoid this we can penalize the contribution of large fragments by instead of using 1 in the precedent formulas, using a value $\lambda$ between 0 and 1. 
\section{Kernel on graphs}
Given the adjacencies matrix and the label matrix based on the one hot encoding of the label: on the rows there are the labels, on the columns the nodes and is 1 if the node has that label, 0 otherwise. \newline
One way to talk about similarity over graphs is to talk about similarity over walk on the graphs, that is a sequence of nodes such that there is a link between each pair of nodes in the list. Since it's a walk it's also possible to go back. We say $W_n(G)$ to refer to all walks in $G$ of length $n$. Two similar graphs will tend to produce similar walks. \newline
We can then define kernel on walks. We could say that we want to compare walks with the same starting and ending labels. To compute this we take the set of walks of length $n$, where the labels of $v_i$ is $l_i$ and the last one $v_{n+1}=l_j$. This is expensive. Moreover, we take the number of walks and sum over all the possible $n$, since we want to take all the possible walks. The coefficient $\lambda_n$ multiplies each walk for a coefficient such that larger walks count less then smaller ones. The idea is then to compute the kernel as a dot product over this feature maps. \newline
We need to compute this efficiently since the number grows exponentially. Luckily there is a property of the adjacency matrix that comes useful: taking $A^n$ we have in each entry the number of walks of length $n$ between each pair of nodes. To know the number of walks between two labels, we just need to multiply left and right by the label matrix. \newline
There is still a problem: we need to compute the exponential of the matrix, but it's efficiently done though the diagonal matrix and eigenvectors. This though is still expensive in case of large graphs, and it can compute similarity only based on the start and end of the path, to look at intermediate node would become much more complex. \newline
What is done now days is to use the Weistfeiler-Lehman graph kernel. This is an efficient graph kernel that relies on isomorphism test: it defines a family of graph tests. \newline
Given two graphs: $G, G'$. They need to have the same number of nodes otherwise they won't be isomorphic. Let $\mathcal{L}(G)$ the set of labels in $G$. Moreover the two set of graphs need to have the same set of nodes otherwise they will not be isomorphic (mind only hte same set, not the node have the same labels). We start by a label somewhere, then cycle for all the edges. For each node $v$ we compute the set $M_i(v)$ of labels in the neighbourhood of the node. Then we create a string out of $M_i(v)$ as the sorted labesl. Then we relabel our node with a number given by the combination of the string and the label of the node. Then we test if the set of labels of the two graphs are still the same. If they are note, then the two graphs are not isomorphic, otherwise if we can end the procedure they (probably are). Now the graph kernel that we can build from this 


\chapter{28-11-2019 Lab}
\chapter{04-12-2019}
\chapter{Deep Networks}
They are networks of neurones. With perceptrons we can learn only linear functions, and even though we saw some non linear models (kernels) that are/were popular and general, they need to be designed appropriately since they require a feature mapping. So in many case not inly we have to choose the kernel, we also have to design it, especially with complex entities. This is called feature engineering and it's usually done with some expert of the field. Moreover our function is not said to be linear if the kernel is not linear, but is a linea combination of kernel functions since the general formula is:
\begin{center}
	$\displaystyle f(x)=\Sum_{i=1}^nc_ik(x,x_j)$
\end{center}
A neural network is a networks of neuron and each of this has a weight that is going to be learned. This architecture is called multilayer perceptron and is a network of perceptrons that is layered where neurones from one layer are attached to the following layer. We distinguish between the input layer without weights, the output layer that gives the output and the hidden layers in the middles distributed hierarchically. \newline
All $\phi$ have parameters $W_1, W_2, ...$ as in slide MLP. Even in kernels we had parameters, but these were fixed, while in neural networks we learn this parameters, which correspond to learning the feature space by learning the weights. This is called representation learning.\newline
The sign function is fine will having just one perceptron. The problem is that if we plug this neuron in the MLP, then we have to take the weights, and for this we need to take the error and then compute the gradient of the error with respect to the weights, which implies deriving the sign function which is either undefined or 0, which leads to never updating. \newline
We could remove the activation function, just take the weighted function which would lead (w.r.t MLP)
\begin{center}
	$\displaystyle f(x)=w^Tw_3w_2w_1x$
\end{center}
That is a linear function, so we need an activation function, which should be non-linear at least in the hidden layers. \newline
We could use something like a threshold (sign), but is not, and that is the sigmoid. 
\begin{center}
	$\displaystyle f(x)=\sigma(w^Tx)=\frac{1}{1+e^{-w^Tx}}$
\end{center}
What is nice is that it is differentiable everywhere, approximately linear in the activation region and 0 (the gradient) only at the extremes.\newline
Suppose we have a MLP, let's assume that hidden layer have sigmoid activation, the only thing missing is the output neurone. \newline
If we need to have binary classification, then we'd like to have one output neurone which says 1 or 0, so we could put a sigmoid activation function (which returns either 1 or 0) and then use a sign function to decide about positive or negative. \newline
If we want to do multi-class classification, we can encode in one hot-encoding and use one output layer per neuron. We take the output layer and transform this in a probability distribution for the classes. To do this we use softmax activation function that similarly to the sigmoid takes $c$ values from $-\infty$ to $+\infty$ and then returns a vector containing values between 0 and 1. This is similar to another layer, but does not have parameters to be learned. The prediction is just taking the highest value. \newline
In case of regression, the real values is not bound, so the typical thing to be done is just not using an activation function and just taking the output. \newline
\section{Representation in MLP}
Any boolean function can be represented by a network with two layer of units.\newline
Any continuous function can be approximated by a network with two layer.\newline
Any arbitrary function can be approximated by a network with three layers.\newline
This should imply that we don't need deep learning. But these don't tell us how many example we need to train the model, and how many neurones per layer. This is clear just by watching the boolean formula. Since each boolean formula can be expressed as conjunctive normal form (CNF), then since each neuron can model a function between AND or OR, then the first layer can just learn the OR part, while the second will learn an AND and we are then.\newline
Let's consider the following example:
\begin{center}
	$\displaystyle (x_1\vee x_3)\wedge(x_1\vee x_4)\wedge(x_2\vee x_3)$
\end{center}
This is easy and works fine. Let's consider though the XOR:
\begin{center}
	$\displaystyle x_1\xor x_2=\x_1\wedge \overline{x_2})\vee(\overline{x_1}\wedge x_2)$
\end{center}
Which noe being linear, it will make the network explode.\newline
What changes is the number of neurones we need per layer for every example. \newline
Deep learning allows to build a composition of processing with the layers, so instead of taking all the XOR at once, it's broken down into pieces.
\section{Training MLP}
In perceptrons the loss function is:
\[l(y,f(x)=
\begin{cases}
	-yf(x) if it's positive\\
	-- otherwise
\end{cases}
=\vert -yf(x)\vert_+
\]
For SVM we had instead:
\[l(y,f(x)=
\begin{cases}
	1-yf(x) if it's positive\\
	-- otherwise
\end{cases}
=\vert 1-yf(x)\vert_+
\]
Even though also in MLP we could use this functions, usually others are used. \newline
For binary classification we use: 
\begin{center}
	$\displaystyle -\Sum\mu p(\mu)log(q(\mu))$
\end{center}
Where $\mu$ are the possible values for the output, which is basically the entropy of using a value for another. \newline
Since the possible output values can only be 1 or 0, then we have:
\begin{center}
	$\displaystyle \mu=1\quad log(f(x))$\\
	$\displaystyle \mu=0\quad log(1-f(x))$
\end{center}
So we can compute the cross entropy (loss function) as:
\begin{center}
	$\displaystyle l(y, f(x))=-[ylog(f(x)+(1-y)log(1-f(x))]$
\end{center}
It also corresponds to maximum likelihood:
\begin{center}
	$\displaystyle \sigma^*-argmax_\sigma(P(x\vert y, \sigma)$
\end{center}
If we don't have prior, then maximum likelyhood is the same as maximum a posteriori:
\begin{center}
	$\displaystyle argmax(P(y\vert x, \sigma)=\frac{P(x\vert y)P(y)}{P(x)}$
\end{center}
\subsection{Multiclass classification}
...\newline
The problem is that up to now it's been pretty easy to define the gradient since we had linear models. \newline
Given the output $\phi_j$ of a previous neurone $j$, and given an actual neurone $l$, and the weights $w_{jl}$ from $j$ to $l$. Let's call $a_l$ the input to the activation function. I want to compute the derivative of the error (given at the end of the network) with respect to these weights. I can use the chain rule and compute the error in something in between $\phi_j$ and $\phi_l$, where there is $a_l$. So I could compute the derivative of the error with respect to something, and then derive something with respect to something else (which is equal to $\phi_j$). Let's call the first derivative $\delta_l$ since we cannot yet compute it for the error is the last.\newline
\ins{1}\newline
In case we are not talking about the output neuron, but an arbitrary, then the $l$ is an hidden neurone, then we have to use again the chain rule. The output of $l$ goes into multiple neurones.\newline
\chapter{05-12-2019}
The derivate of the error with respect to $\phi_j$ cannot be computed inside of the module cause this one cannot know the error, so it's given from above. Once the error has been computed, then the error is back propagated. \newline
So it needs to compute the derivative of:
\newcommand{\derivative}[2]{\frac{\delta#1}{\delta#2}}
\begin{itemize}
	\item The error with respect to its weights, which is gonna be used
		\begin{center}
			\begin{center}
				$\displaystyle \derivative{E}{W_j}=\derivative{E}{\phi_j}\derivative{\phi_j}{W_j}=\derivative{E}{\phi_j}\derivative{F_j(\phi_j}{a}$
			\end{center}
		\end{center}
	\item The error with respect to its inputs, which is gonna be sent downwards. 
\end{itemize}
Even though $F$ usually is just a sigmoid, it can become pretty complex.\newline
One advantage of kernel machine is that they are convex, so there is no local minima: or there is a global optimal or there is. While kernel machines tend to be more secure, deep networks is more creative and shows substantial advantages such in computer vision. Moreover while deep learning works on big quantities of data, kernel machine works more out of the shelf, but usually requires a field expert.\newline
Deep networks are not guaranteed to be convex and so to reach a global minimum. \newline
When using gradient descend we need to know when to stop. Suppose we really reach the global minimum without knowing. One problem is that since deep networks have a lot of parameters and example, tend to overfit, and we want to avoid this: we don't want to necessarily get 0 error. What get's done is to plot, over the number of iterations, the training error, which should go down, the error on a separate validation set, and then we check how the validation error behaves and we don't take the network with the minimum training error, but the one with the minimum validation error. We stop when the validation error starts to increase instead of the training error (which should not go up). This is called early stopping. \newline
If the error goes up and down means that we are taking steps too big, that is the learning rate is too high. \ins{1}.\newline
Vanishing gradient: so since the sigmoid is no good for gradients, then we use a rectifier, that is 0 when x<0 and x for all other values. \newline
\subsection{Regularization}
We want to penalize objects that are too complex. Since weights is what change ecc, then we can use the norm of the weights. 
Norm-1 is called sparsifying norm. The nice thing of having vertices is that probably the weights and the values are going to touch on a vertix which correspond to having one weight to 0.
\section{Initialization}
Starting with all weights at 0 can imply that some neurones follow the same path and learn the same stuff. In order to avoid redundancy is better to initialize randomly the values, but without big values since it could go immediately in saturated areas.\newline
Another key aspect it how to deal with iterations: each iteration means back propagation and updting the weights. The update can be decided in two ways:
\begin{itemize}
	\item Batch gradient: expect all example and then compute gradient descent. This is too slow fro big datasets.
	\item In this case the information given can be too different from all the rest and take a path that is different.
	\item mini-batches: gradient descent on a finite number of example. 
\end{itemize}
Gradient descent does not work very well because of the complexity of setting the learning rate: could be too much or maybe it should vary during the execution. \newline
Our weight are update with velocity, which is the gradient+ the value at the previous iteration. 

\chapter{18/12/2019}
Slide 37, per quest'ultimo passaggio sono necessarie delle label.\newline
We saw that there are a number of tricks that allow to train models directly, for example batches, and there are a lot of (labeled) data. Moreover we need really a lot of power to train the models (this is why once there was no Deep Learning).\newline
There are also other ways: 
\begin{itemize}
	\item Supervised pre-training: instead of auto-encoder mode we have labels.
	\item Transfer learning (or domain transformation): it's not given that there is a lot of data for all the task, but if for example we learn to categorise faces in general, then we can specify the model on recognising faces of people in the university. To this we have to retrain, that is tune, on the specific set we need. 
	\item multi-level supervision: for example instead of predicting as output if male or female, we could have the results midway through some other task.
\end{itemize} 
There are many other architectures other then auto-encoder:
\begin{itemize}
	\item Convolutional networks for exploiting local correlations, for example image recognition. Thi network are conceived to exploit local correlation, so the input should have some spacial property. It works by learning filters, that is weight matrixes. This are called convolution filters, and we can have multiple, even in one layer. This allow to avoid having one weight for each pixel which is exponential (like neurones), but it works on all the image.
	\item Recurrent for sequential prediction: predicting a label from a sequence of observations. One problem with the networks we have seen so far is that they require fixed size input (also convolutional can work with differente size images, but they don't work well). For example if we want to translate a paragraph, we cannot assume that all paragraph have the same length. The idea is to process the data using a template which is repeated.
	\item Probabilistic generative models (for example Boltzmann): they can predict new stuff.
	\item Generative adversarial networks: generate new adversarial entities.
\end{itemize}
\chapter{Unsupervised learning}
What we do when we have little labelled data. \newline
To initialise the mean of the cluster choose randomly. \newline
\chapter{19/12/2019}
Slide 13, solita costruzione della likelihood, da log del prodotto si passa a somma di log. \newline
Slide 21 il grafico non è monotono\newline
































