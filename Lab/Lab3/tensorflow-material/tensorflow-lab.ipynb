{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of tensorflow-lab-reviewed.ipynb","provenance":[{"file_id":"1UA2QHoeRflU_2Cd4WF6i5JSp-1e2Bxa_","timestamp":1576057366542},{"file_id":"1E6WOAjkYRZGYsofpxryN91knHD3vHIbM","timestamp":1575563717680}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rX8mhOLljYeM"},"source":["<h1 align=center style=\"color: #005496; font-size: 4.2em;\">TensorFlow</h1>\n","<h2 align=center>Laboratory on Neural Networks</h2>\n","\n","***\n","\n","\n","\n","## Introduction\n","\n","A few years back, Google has open-sourced its machine learning framework: **TensorFlow**. Since then, it has become one of the most used tools in machine learning, both for research purposes and for production systems. TensorFlow is very flexible and can handle a wide range of neural architectures.\n","\n","In this lecture we will see how we can build a simple Neural Network capable of categorizing images from the *MNIST* dataset into digits. This tutorial \n","is a mix of some guides hosted on the [tensorflow website](https://www.tensorflow.org/tutorials).\n","\n","This lecture assumes familiarity with the Python programming language and the libraries we have seen in the previous lecture: Numpy, Matplotlib and Scikit-learn.\n","\n","\n","## Google Colab\n","\n","Google Colab is a  cloud service based on Jupyter  that enables to execute notebooks in the cloud, using resources from Google's datacenters.\n","Google even provides limited access to GPU, in order to accelerate tasks such as \n","deep learning.\n","\n","## References\n","\n","- https://www.tensorflow.org\n","- https://www.tensorflow.org/tutorials/quickstart/beginner\n","- https://www.tensorflow.org/tutorials/quickstart/advanced\n","- http://yann.lecun.com/exdb/mnist/"]},{"cell_type":"markdown","metadata":{"id":"ig3OtaU8ht6n","colab_type":"text"},"source":["# Object-oriented programming in Python\n","Before start playing with tensorflow we need to briefly introduce objects in Python.\n"]},{"cell_type":"markdown","metadata":{"id":"zOhvjFGLZOhU","colab_type":"text"},"source":["\n","\n","## Classes and Objects\n","Python is an object oriented programming language.\n","Almost everything in Python is an object, with its properties and methods.\n","\n","An object is a combination of variables and functions used to perform \n","operations over them. Each object is an instance of a class.\n","\n","A class is like an object constructor, or a \"blueprint\" for creating objects.\n","A class defines both object's variables (usually referred as `attributes`) and\n","funtions (referred as `methods`) that will operate over them.\n","\n","Among many others, there exists a special method, the __\\_\\_init\\_\\___ method.\n","In the __\\_\\_init\\_\\___ method  we define and initialize the \n","attributes of the objetcs we will create starting from the class.\n","\n","As the language mandates, the first argument of each method of a class \n","is `self`, which is a pointer to the instance of the class itself.\n","This enables to perform computation involving attributes:"]},{"cell_type":"code","metadata":{"id":"NM3kZaEeaLGZ","colab_type":"code","outputId":"83c86541-3ad5-4e36-9943-88fd78b18aae","colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"status":"ok","timestamp":1576070697857,"user_tz":-60,"elapsed":732,"user":{"displayName":"Luca Erculiani","photoUrl":"","userId":"16298406135568145515"}}},"source":["class Person: # the class definition\n","  def __init__(self, p_name, p_age):\n","    self.name = p_name # create some attributes\n","    self.age = p_age\n","\n","  def hello(self, greetings=\"Hello\"): # define a method\n","    print(greetings + \" my name is \" + self.name)\n","\n","p1 = Person(\"John\", 36) # create object from class Person\n","print(p1.age)\n","p1.hello(greetings=\"Hi\")\n","p1.hello()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["36\n","Hi my name is John\n","Hello my name is John\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZUy4Rpg3h5dc","colab_type":"text"},"source":["Then we can reuse the class to create other persons"]},{"cell_type":"code","metadata":{"id":"RHX5Zb4vgoxV","colab_type":"code","outputId":"67fab9b2-a92f-4141-86f2-6af4ec26203b","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1576070698182,"user_tz":-60,"elapsed":1041,"user":{"displayName":"Luca Erculiani","photoUrl":"","userId":"16298406135568145515"}}},"source":["p2 = Person(\"Alice\", 27)\n","\n","print(p2.age)\n","p2.hello()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["27\n","Hello my name is Alice\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5iChlfctc7uj","colab_type":"text"},"source":["## Inheritance\n","\n","Inheritance enables us to define a class that inherits all the methods and attributes from another class.\n","\n","The __parent__ class is the class being inherited from, also called base or super class.\n","\n","The __child__ class is the class that inherits from another class, also called derived  or sub class.\n","\n","We can specify the parent class by putting its name \n","in brackets after the name of the child class.\n","\n","In the child's __\\_\\_init\\_\\___ method we must call the parent's __\\_\\_init\\_\\___\n","method, in order to initialize the parent correctly. This can \n","be done by using the `super()` function"]},{"cell_type":"code","metadata":{"id":"3pXQeJOLdIVi","colab_type":"code","outputId":"b75ac45d-c4fc-4a75-f23c-53e766593ed8","colab":{"base_uri":"https://localhost:8080/","height":86},"executionInfo":{"status":"ok","timestamp":1576070698184,"user_tz":-60,"elapsed":1031,"user":{"displayName":"Luca Erculiani","photoUrl":"","userId":"16298406135568145515"}}},"source":["class Student(Person): # Student inherits from Person\n","  def __init__(self, name, age, year):\n","    super().__init__(name, age) # call __init__ method of the parent class\n","    self.graduationyear = year\n","\n","  def welcome(self):\n","    print(\"Welcome\", self.name, \"to the class of\", self.graduationyear) \n","\n","s1 = Student(\"Bob\", 24, 2021)\n","\n","print(s1.age) # inherited from Person\n","s1.hello(greetings=\"Hi\") # inherited from Person \n","\n","print(s1.graduationyear)\n","s1.welcome()\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["24\n","Hi my name is Bob\n","2021\n","Welcome Bob to the class of 2021\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hiH7AC-NTniF"},"source":["# TensorFlow\n","\n","Let's dive into how to declare and train a Neural Network with *TensorFlow*.\n","\n","Instruct Google Colab to load TensorFlow __version 2__:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ioLbtB3uGKPX","outputId":"b5ca7ce0-223c-4e9f-f2ca-9623b111fa84","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1576070698185,"user_tz":-60,"elapsed":1020,"user":{"displayName":"Luca Erculiani","photoUrl":"","userId":"16298406135568145515"}}},"source":["try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QS7DDTiZGRTo"},"source":["Import TensorFlow  into your program:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0trJmd6DjqBZ","colab":{}},"source":["import tensorflow as tf\n","\n","tf.random.set_seed(0) # set seed for reproducibility, still not deterministic if a GPU is used\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rWFF6_-fgniz","colab_type":"text"},"source":["As it was for the numpy library (which we used in the last tutorial), \n","even  TensorFlow has its basic data structure, called __Tensor__. \n","A Tensor is the equivalent of the numpy array, a fixed-size structure containing\n","an ordered collection of homogeneous data. \n"]},{"cell_type":"code","metadata":{"id":"jqAwzNvEjBbf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"8e5dd838-4256-4a60-d36f-1b7ee067c3da","executionInfo":{"status":"ok","timestamp":1576070702759,"user_tz":-60,"elapsed":5574,"user":{"displayName":"Luca Erculiani","photoUrl":"","userId":"16298406135568145515"}}},"source":["t = tf.convert_to_tensor([1,2,3,4])\n","t"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: id=0, shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"tBohavodj53B","colab_type":"text"},"source":["There are many ways to declare tensors, many of them quite \n","similar to numpy:"]},{"cell_type":"code","metadata":{"id":"v9WbnP7Dj_KI","colab_type":"code","outputId":"c3d436ce-2efc-4474-ff8e-1ed44751e7e2","colab":{"base_uri":"https://localhost:8080/","height":86},"executionInfo":{"status":"ok","timestamp":1576070702761,"user_tz":-60,"elapsed":5564,"user":{"displayName":"Luca Erculiani","photoUrl":"","userId":"16298406135568145515"}}},"source":["import numpy as np\n","a = np.arange(5)\n","t = tf.range(5)\n","\n","print(a)\n","print(t)\n","\n","print(a.shape)\n","print(t.shape)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[0 1 2 3 4]\n","tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int32)\n","(5,)\n","(5,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Te9XRd9NkPUK","colab_type":"text"},"source":["The library offers easy ways to switch between Tensors and numpy arrays:"]},{"cell_type":"code","metadata":{"id":"3Iyl8Hm7kVvv","colab_type":"code","outputId":"79385eb4-9ae8-4a93-e7d4-9a402b6f8ea4","colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"status":"ok","timestamp":1576070702762,"user_tz":-60,"elapsed":5553,"user":{"displayName":"Luca Erculiani","photoUrl":"","userId":"16298406135568145515"}}},"source":["an_array = np.array([1,2,3])\n","a_tensor = tf.convert_to_tensor(an_array) # from array to tensor\n","\n","another_array = a_tensor.numpy() # from tensor to array\n","\n","print(an_array)\n","print(a_tensor)\n","print(another_array)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[1 2 3]\n","tf.Tensor([1 2 3], shape=(3,), dtype=int64)\n","[1 2 3]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZpPwGPHTR5mI","colab_type":"text"},"source":["Tensors can be directly used within the numpy library"]},{"cell_type":"code","metadata":{"id":"TYX6g67HR4vK","colab_type":"code","outputId":"7695fba3-6b6f-47bb-f7b2-2b871d9028ed","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1576070705029,"user_tz":-60,"elapsed":7808,"user":{"displayName":"Luca Erculiani","photoUrl":"","userId":"16298406135568145515"}}},"source":["np.sqrt(an_array * a_tensor)"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1., 2., 3.])"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7NAbSZiaoJ4z"},"source":["## MNIST dataset\n","\n","*MNIST* is a dataset containing labelled images of handwritten digits. This dataset is similar to the *digits* dataset we have seen in the previous lecture, but the images here are larger, 28 by 28 pixels.\n","\n","We can use TensorFlow directly to download and read the MNIST data:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JqFRS6K07jJs","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"2ba02369-435a-4913-9336-a0e7b53af0f9","executionInfo":{"status":"ok","timestamp":1576070705640,"user_tz":-60,"elapsed":8407,"user":{"displayName":"Luca Erculiani","photoUrl":"","userId":"16298406135568145515"}}},"source":["mnist = tf.keras.datasets.mnist\n","\n","(data_train, targets_train), (data_test, targets_test) = mnist.load_data()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Reu61roKtyuu","colab_type":"text"},"source":["The data instances in MNIST are images represented by  arrays contaning 784 numbers between 0 and 255, i.e. the gray scale relative intensity of the pixels (0 means white, 255 means black). Later we will  normalize them, obtaining values between 0 and 1. The shape of the loaded data is\n","(number of images, 28, 28).\n","\n","<img src=\"https://www.tensorflow.org/images/MNIST-Matrix.png\" width=\"600px\" />\n","\n","***\n","\n","MNIST already comes with a train-test split. The training data is a *numpy array* of 60000 images, while the test data contains 10000 images.\n","\n","<img src=\"https://i.imgur.com/vNWCOld.png\" width=\"600px\"/>\n","\n","\n","***\n","\n","In the current version of Tensorflow the labels are encoded \n","as categorical values (integers from 0 to 9). For this tutorial we will use the so-called\n","\"one-hot\" representation. Each label `y` is a vector of 10 0-1 elements, in which only the position associated with the true digit is set to 1, the others are set to 0. Thus, the training labels will be encoded as a  tensor of shape (number of images, 10).\n","\n","<img src=\"https://i.imgur.com/4sgGtT1.png\" width=\"650px\" />"]},{"cell_type":"markdown","metadata":{"id":"FRuNej-iCUol","colab_type":"text"},"source":["Once loaded, we can visualize the data using Matplotlib"]},{"cell_type":"code","metadata":{"id":"m5p4xpKxwNCA","colab_type":"code","outputId":"07dc616c-a004-47db-e37a-844a635e825b","colab":{"base_uri":"https://localhost:8080/","height":292},"executionInfo":{"status":"ok","timestamp":1576070705641,"user_tz":-60,"elapsed":8397,"user":{"displayName":"Luca Erculiani","photoUrl":"","userId":"16298406135568145515"}}},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","plt.gray()\n","plt.matshow(255 - data_train[0]) # 255 - x simply inverts the fading direction of the image\n","plt.show()"],"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAQEAAAECCAYAAAD+eGJTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZ5QAEBwlAARHCQDBUQJAcP8Hyn4ZAKmjRdEAAAAASUVORK5CYII=\n","text/plain":["<Figure size 288x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"C4sJlBXgxw5w","colab_type":"text"},"source":["While the label is:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6HfdXRl9eEAz","outputId":"e5f90d01-4da1-44a3-fe8a-2cd91d601694","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1576070705644,"user_tz":-60,"elapsed":8388,"user":{"displayName":"Luca Erculiani","photoUrl":"","userId":"16298406135568145515"}}},"source":["targets_train[0]"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"TX2zXK0RY7ym","colab_type":"text"},"source":["We can use the `tf.one_hot` function in order to obtain the one hot encoding"]},{"cell_type":"code","metadata":{"id":"vIQ7bNbgx8V4","colab_type":"code","outputId":"114a96f5-3da4-4555-fc14-7e68c7360dc6","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1576070705646,"user_tz":-60,"elapsed":8378,"user":{"displayName":"Luca Erculiani","photoUrl":"","userId":"16298406135568145515"}}},"source":["one_hot = tf.one_hot(targets_train[0], 10) # we must specify the number of classes, 10 for MNIST\n","one_hot"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: id=12, shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"MaWKcl4hUnRG","colab_type":"text"},"source":["Let's cast the data to floating points, normalize and reshape it (we need to add an empty dimension at the end, in order to use it later to train  our neural networks)"]},{"cell_type":"code","metadata":{"id":"kX4_JkHTUmkk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"7b1bcbff-fe08-43d9-b691-c5835f4271e4","executionInfo":{"status":"ok","timestamp":1576070706150,"user_tz":-60,"elapsed":8869,"user":{"displayName":"Luca Erculiani","photoUrl":"","userId":"16298406135568145515"}}},"source":["                 \n","x_train = tf.cast(data_train, tf.float32) / 255.0\n","x_test = tf.cast(data_test, tf.float32) / 255.0\n","\n","# Add a fourth dimension\n","x_train = x_train[..., tf.newaxis]\n","x_test = x_test[..., tf.newaxis]\n","\n","x_train.shape"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([60000, 28, 28, 1])"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"MgnT0heLxAlB","colab_type":"code","colab":{}},"source":["y_train = tf.one_hot(targets_train, 10)\n","y_test = tf.one_hot(targets_test, 10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HyK6IlFpyDo0","colab_type":"text"},"source":["## Softmax Regression\n","\n","The first thing we are going to implement using TensorFlow is a simple softmax regression model to predict the digits from the images. Softmax regression is a generalization of Logistic Regression to multi-class classification.\n","\n","In softmax regression we define a linear model for each possible class. These models provide evidence supporting the image `x` being of the particular class `i`:\n","\n","\\begin{equation}\n","    \\theta_i = \\sum_{1 \\le j \\le 784} W_{i,j} \\cdot x_j + b_i\n","\\end{equation}\n","\n","A softmax function is then applied to each linear model in order to get the probability $y_i$ of the image being of class `i`:\n","\n","\\begin{equation}\n","    y_i = \\text{softmax}(\\theta_i) = \\frac{e^{\\theta_i}}{\\sum_{j} e^{\\theta_j}}\n","\\end{equation}\n","\n","This is a schematic view of the operations we are a doing:\n","\n","<img src=\"https://www.tensorflow.org/images/softmax-regression-scalargraph.png\" width=\"400px\" />\n","\n","***\n","\n","Spelling out the equations of our model:\n","\n","<img src=\"https://www.tensorflow.org/images/softmax-regression-scalarequation.png\" width=\"400px\" />\n","\n","***\n","\n","The vectorized form:\n","\n","<img src=\"https://www.tensorflow.org/images/softmax-regression-vectorequation.png\" width=\"400px\" />\n","\n","***\n","\n","The vectorized equation is therefore the following:\n","\n","\\begin{equation}\n","    y = \\text{softmax}(W \\cdot x + b)\n","\\end{equation}\n","\n","Where $y$ is a $1 \\times 10$ column vector, $W$ is a $10 \\times 784$ matrix, $x$ is a $1 \\times 784$ column vector and $b$ is a $1 \\times 10$ column vector. While we used column vectors for ease of exposition, in the implementation is much more convenient to use row vectors for $x$, $b$ and $y$. Hence, the matrix `W` in the code below will be of shape (784, 10). Let's now dive into the definition of this perceptron with TensorFlow.\n","\n","First we need to import some objects that we will use to compose\n","our models:"]},{"cell_type":"code","metadata":{"id":"ARw2Yx5zCjVV","colab_type":"code","colab":{}},"source":["from tensorflow.keras import Model\n","from tensorflow.keras.layers import Softmax, Flatten"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c4LSQXhyDZbR","colab_type":"text"},"source":["Then we can start defining our custom perceptron.\n","\n","In Tensorflow, in order to create a custom layer or network we must\n","extend the class `tensorflow.keras.Model`, by defining\n","all the weights (or layers) and the operations of our model.\n","Our network will be a child class of  `tensorflow.keras.Model`, inheriting \n","all the capabilities of that class, for instance the ability to \n","perform automatic gradient computation over its parameters.\n","\n","Let's define our perceptron. In the __\\_\\_init\\_\\___ method we need to declare our matrix of weights `self.W` and biases `self.b`, plus two more layers: a  `Flatten` layer, which is used to trasform\n","images, represented as 28x28 matrices, to vectors of 784 elements, \n","and a `Softmax` layer that, as the name suggests, will compute the softmax over \n","the output of the perceptron.\n","\n","In the __call__ method we use the components we declared in order to\n","compose the series of operations needed to perform inference over \n","a batch of examples `x`, represented as a tensor whose shape at runtime will be \n","(batch size, 28, 28, 1). First we apply the flatten layer, in order to obtain\n","a batch with shape (batch size,  784), then we perform the matrix multiplication\n","with `self.W` and the addition of the bias vector `self.b`.\n","\n","The result of the matrix multiplication has shape (batch size, 10), while \n","the bias vector has shape (1, 10). In order to execute this addition ,\n","tensorflow performs an automatic  *broadcasting* of the biases by repetition, in order\n","to obtain the same shape of the other operand. You can find \n","all the broadcasting rules in the [documentation](https://www.tensorflow.org/xla/broadcasting).\n"]},{"cell_type":"code","metadata":{"id":"GdTehAZfyP1-","colab_type":"code","colab":{}},"source":["class MnistPerceptron(Model): # inherit from Model\n","  def __init__(self):\n","    super().__init__() # initialize Model\n","    self.flatten = Flatten() # used to flatten pixels of images\n","    self.W = tf.Variable(tf.zeros([784, 10])) # declare weights with shape :(748, 10)\n","    self.b = tf.Variable(tf.zeros([1, 10]))   # declare biases, with shape :(1, 10)\n","    self.softmax = Softmax()\n","    \n","\n","  def call(self, x,training=False): \n","    # the  training argument is unused in this model, we will need it later\n","                                                  \n","    flat = self.flatten(x) # flatten images   \n","            \n","    multiplied = tf.matmul(flat, self.W) # matmul, output shape : (batch, 10)\n","    # we can equivalently do:\n","    #multiplied = tf.transpose(tf.matmul(tf.traspose(self.W), tf.traspose(flat)))\n","\n","    fwded = multiplied + self.b # broadcast self.b to (batch, 10) and add   \n","\n","    prob = self.softmax(fwded) # softmax              \n","    return prob\n","\n","# Create an instance of the model\n","perceptron = MnistPerceptron()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IqQ7GN39ws5T","colab_type":"text"},"source":["## Training\n","\n","After we define the model, we want to train it using the MNIST data, using gradient descent. To do so, we first need to define a **loss function**. For this task we are going to use the *cross-entropy* loss:\n","\n","\\begin{equation}\n","    H_{y}(\\hat{y}) = - \\sum_i y_i \\log(\\hat{y}_i)\n","\\end{equation}\n","\n","Where $\\hat{y}$ is the distribution predicted by our model and $y$ is the true probability distribution of the classes (i.e. the one-hot vector).\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yEQJ-UViIKke","colab":{}},"source":["perceptron_loss = tf.keras.losses.CategoricalCrossentropy()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wDEyPUplQGwO","colab_type":"text"},"source":["Then we must create a `Dataset`, starting from the MNIST data, that we will use to train end evaluate our models. A Dataset is an object that can be used to \n","organize our raw data in batches of examples.\n","\n","Let's create a train and test datasets. Then we will be able to iterate over them, obtaining a batch at each iteration that we will use to perform gradient descent. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8Iu_quO024c2","colab":{}},"source":["train_ds = tf.data.Dataset.from_tensor_slices(\n","    (x_train, y_train)).shuffle(10000).batch(100)\n","\n","test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(50)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dsjpYiCmIQaf"},"source":["Choose an optimizer for training. \n","In this case we will use Stocastic Gadient Descent (SGD) with a leraning rate of 0.01"]},{"cell_type":"code","metadata":{"id":"TQtcssMDRDwm","colab_type":"code","colab":{}},"source":["perceptron_optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QhNUPd-qIwCc"},"source":["Select metrics to measure the loss and the accuracy of the model. These metrics accumulate the values over different batches and then print the overall result."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7RpViF5HIyFh","colab":{}},"source":["train_loss_metric = tf.keras.metrics.Mean()\n","train_accuracy_metric = tf.keras.metrics.CategoricalAccuracy()\n","\n","test_loss_metric = tf.keras.metrics.Mean()\n","test_accuracy_metric = tf.keras.metrics.CategoricalAccuracy()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3MUTz4YNRi8i","colab_type":"text"},"source":["Then we can define a series of functions that we will use to train our model.\n","\n","Our training framework is an iterative procedure that starts by sampling a\n","batch of examples from the training set, coupled with their labels. \n","The data is fed to the model, whose predictions are matched with the\n","true labels via a loss function. Then the gradient of the loss with\n","respect to the parameters of  the model is computed, and used to \n","update the parameters (following the directions in which it decreases).\n","\n","The tensorflow library is able to automatically compute these \n","operations. In order to\n","do this we must create a disposable object, called __tape__.\n","This tape keeps track of the operations performed during inference\n","and computes the gradient with respect the loss function. \n","The gradient is then passed to the optimizer in order to\n","perform the update. \n","\n","As last operation we update our accumulators to keep track of the \n","loss and accuracy metrics.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OZACiVqA8KQV","colab":{}},"source":["def train_step(images, labels, model, loss_fn, optimizer):\n","  with tf.GradientTape() as tape: # all the operations within this scope will be recorded in tape\n","    predictions = model(images, training=True)\n","    loss = loss_fn(labels, predictions)\n","  gradients = tape.gradient(loss, model.trainable_variables)\n","  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","  train_loss_metric(loss)\n","  train_accuracy_metric(labels, predictions)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dhqpAs7JYsM5","colab_type":"text"},"source":["We can then use our train step in order to perform training, by creating a training loop that iterates for a given number of times \n","(usually referred as *epochs*) trought\n","the dataset, printing the metrics at the end of each step."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XhiqfKANLw4-","colab":{}},"source":["from datetime import datetime\n","\n","def train_loop(epochs, train_ds, model, loss_fn, optimizer):\n","  for epoch in range(epochs):\n","      # reset the metrics for the next epoch\n","    train_loss_metric.reset_states()\n","    train_accuracy_metric.reset_states()\n","\n","    start = datetime.now() # save start time \n","    for images, labels in train_ds:\n","      train_step(images, labels, model, loss_fn, optimizer)\n","\n","    template = 'Epoch {}, Time {}, Loss: {}, Accuracy: {}'\n","    print(template.format(epoch+1,\n","                          datetime.now() - start,\n","                          train_loss_metric.result(),\n","                          train_accuracy_metric.result()*100))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nc6SDaaMRvo7","colab_type":"text"},"source":["Now we are ready to perform training"]},{"cell_type":"code","metadata":{"id":"PaYGTKTnUXrd","colab_type":"code","outputId":"87aed103-8b7b-4e8e-91b5-db887746d41a","colab":{"base_uri":"https://localhost:8080/","height":191},"executionInfo":{"status":"ok","timestamp":1576070809205,"user_tz":-60,"elapsed":80704,"user":{"displayName":"Luca Erculiani","photoUrl":"","userId":"16298406135568145515"}}},"source":["EPOCHS = 10\n","train_loop(EPOCHS, train_ds, perceptron, perceptron_loss, perceptron_optimizer)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Epoch 1, Time 0:00:08.748695, Loss: 1.1449335813522339, Accuracy: 80.3550033569336\n","Epoch 2, Time 0:00:07.976085, Loss: 0.6415689587593079, Accuracy: 85.72833251953125\n","Epoch 3, Time 0:00:07.845040, Loss: 0.5356608629226685, Accuracy: 87.04499816894531\n","Epoch 4, Time 0:00:07.905150, Loss: 0.48453623056411743, Accuracy: 87.8066635131836\n","Epoch 5, Time 0:00:07.948479, Loss: 0.45318278670310974, Accuracy: 88.3066635131836\n","Epoch 6, Time 0:00:07.982059, Loss: 0.43141448497772217, Accuracy: 88.69332885742188\n","Epoch 7, Time 0:00:07.944890, Loss: 0.41525769233703613, Accuracy: 88.97167205810547\n","Epoch 8, Time 0:00:07.874499, Loss: 0.4026186764240265, Accuracy: 89.22000122070312\n","Epoch 9, Time 0:00:07.900007, Loss: 0.39235949516296387, Accuracy: 89.45000457763672\n","Epoch 10, Time 0:00:07.942112, Loss: 0.38381922245025635, Accuracy: 89.62000274658203\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oXjv7H5cf7Hl","colab_type":"text"},"source":["## Evaluation\n","\n","Let's now evaluate our model on the test set. We need to define another couple of functions, in order to compute the metrics over each batch and then to \n","aggregate the results."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xIKdEzHAJGt7","colab":{}},"source":["def test_step(images, labels, model, loss_fn):\n","  predictions = model(images, training=False)\n","  t_loss = loss_fn(labels, predictions)\n","\n","  test_loss_metric(t_loss)\n","  test_accuracy_metric(labels, predictions)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"X_DILkJpe89t","colab":{}},"source":["def test_loop(test_ds, model, loss_fn):\n","    # reset the metrics for the next epoch\n","  test_loss_metric.reset_states()\n","  test_accuracy_metric.reset_states()\n"," \n","  for test_images, test_labels in test_ds:\n","    test_step(test_images, test_labels, model, loss_fn)\n","\n","  template = 'Test Loss: {}, Test Accuracy: {}'\n","  print(template.format(test_loss_metric.result(),\n","                        test_accuracy_metric.result()*100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cmch1gnBTU8r","colab_type":"text"},"source":["Then we can perform evaluation of our trained perceptron"]},{"cell_type":"code","metadata":{"id":"9d4V_PgAgCA_","colab_type":"code","outputId":"e7fecc4f-1549-48b6-cb47-74524128c834","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1576070957054,"user_tz":-60,"elapsed":2366,"user":{"displayName":"Luca Erculiani","photoUrl":"","userId":"16298406135568145515"}}},"source":["test_loop(test_ds, perceptron, perceptron_loss)"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Test Loss: 0.36067456007003784, Test Accuracy: 90.36000061035156\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PHAi4G_AgLia","colab_type":"text"},"source":["Good! Our model works (sort of). These performance on MNIST are way too low. The state-of-the-art on this dataset has an accuracy of $0.9979$. Let's try to achieve a better accuracy by refining a bit our model."]},{"cell_type":"markdown","metadata":{"id":"o2yOY09Ag_0x","colab_type":"text"},"source":["## Deep architectures\n","\n","<p>\n","We are now going to define a deep neural network capable of classifying digits much better than the shallow softmax we used before. We are going to use two convolutional layers alternated with 2 max pool layers, followed by a fully connected layer regularized with dropout, and finally we'll get predictions using again a softmax layer. Here you see a schematic view of the architecture (bottom up). \n","</p>\n","\n","<img src=\"https://i.imgur.com/fra8hbB.png\" style=\"width: 350px\" />\n","\n","***\n","\n","### Fully Connected Layer\n","In a fully connected layer each neuron is connected with each neuron of the previous layer. This is the most straightforward way to implement a Neural Network.\n","\n","<img src=\"https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1547672259/3_qwv5gr.png\" width=\"750px\" />\n","\n","***\n","\n","\n","### Convolutional layer\n","\n","Convolutional Neural Networks are based on a method for extracting meaningfull features from images, exploiting locality and parameter sharing between \"close\" pixels. In particular, a convolutional layer takes as input a matrix and divides it into smaller \"patches\". It outputs a number of features for each patch. The resulting tensor will be similar to the one in the following picture.\n","\n","<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/68/Conv_layer.png\" width=\"400px\" />\n","<img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides.gif\" width=\"250px\" />\n","\n","Applying a convolution reduces the height and the width of the \n","input image. This effect is not always desirable. \n","In order to avoid it, we must add some empty data around our images.\n","This data (usually containing zeros) is called *padding*\n","\n","\n","\n","<img src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/same_padding_no_strides.gif\" width=\"400px\" />\n","\n","\n","***\n","\n","### Max pool layer\n","\n","The max pool layer is a filter that is used to reduce the number of the input dimensions between two convolutional layers. It is useful for dimensionality reduction, it avoids overfitting, etc. The following is a simple example:\n","\n","<img src=\"http://www.wildml.com/wp-content/uploads/2015/11/Screen-Shot-2015-11-05-at-2.18.38-PM.png\" width=\"400px\" />\n","\n","***\n","\n","### ReLU layer\n","\n","Rectified Linear Unit (ReLU) is a particular activation function of the type: $max(0, \\theta)$. ReLUs have become very wide spread thanks to the fact, differently from sigmoids, that they do not saturate the gradient when used with backpropagation. This has enabled neural networks to become deeper and to increase their representational power. The following image compares several activation functions and their gradients.\n","\n","<img src=\"https://d9johdpvmzlgp.cloudfront.net/images/Blog/af.jpg\" width=\"700px\" />\n","\n","***\n","\n","### Dropout\n","\n","Dropout is a regularization technique for neural networks. When using dropout, we set a probability for dropping random nodes from the networks at each gradient descent step. In this way, the network is going to learn a more robust model, not relying too much on a single node for a prediction. In some sense, using dropout we are \"sampling\" a lot of different networks and learning them, and finally we are left with the \"average\" network, which is less susceptible to outliers. Dropouts also helps in avoiding overfitting.\n","\n","<img src=\"https://pgaleone.eu/images/dropout/dropout.jpeg\" width=\"400px\" />\n","\n","***"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BPZ68wASog_I"},"source":["Let's start by defining two modules, one encoding a convolutional layer, another for the fully connected layer."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"h3IKyzTCDNGo","colab":{}},"source":["class MnistConvolutional(Model):\n","  def __init__(self, in_channels, out_channels, size):\n","    super().__init__() # setup the moedl basic functionalities (mandatory)\n","    initial = tf.random.truncated_normal([size, size, in_channels, out_channels], stddev=0.1)\n","    self.filters = tf.Variable(initial) # create weights for the filters\n","\n","  def call(self, x):\n","    res = tf.nn.conv2d(x, self.filters, 1, padding=\"SAME\")\n","    return res\n","\n","class MnistFullyConnected(Model):\n","  def __init__(self, input_shape, output_shape):\n","    super().__init__() # initialize the model\n","    self.W = tf.Variable(tf.random.truncated_normal([input_shape, output_shape], stddev=0.1)) # declare weights \n","    self.b = tf.Variable(tf.constant(0.1, shape=[1, output_shape]))  # declare biases\n","    \n","  def call(self, x):\n","    res = tf.matmul(x, self.W) + self.b \n","    return res\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wDckUtq7FGsc","colab_type":"text"},"source":["Then we can define our complete network.\n","\n","Note that, due to the presence of the dropout,\n","in this case the behavior of the network at evalution phase\n","is different than the one at training phase.\n","\n","During evaluation, the dropout layer must rescale the output \n","instead of randomly dropping part of it. For this reason, \n","the dropout at inference time takes in input an argument `training`\n","that controls the behavior of the layer. We can pass to it the \n","value our network receives from the caller, enabling us to \n","control it from the outside (see `treain_step` and `test_step` functions)."]},{"cell_type":"code","metadata":{"id":"vidQ7PAyVLDa","colab_type":"code","colab":{}},"source":["from tensorflow.keras.layers import MaxPool2D, Dropout\n","\n","class MnistDeepModel(Model):\n","  def __init__(self):\n","    super().__init__()                            #input shape: (batch, 28, 28, 1)\n","    self.conv1 = MnistConvolutional(1, 32, 5)  # out shape : (batch, 28, 28, 32)\n","    self.pool1 = MaxPool2D([2,2])                 # out shape : (batch, 14, 14, 32)\n","    self.conv2 = MnistConvolutional(32, 64, 5) # out shape : (batch, 14, 14, 64)\n","    self.pool2 = MaxPool2D([2,2])                 # out shape : (batch, 7, 7, 64)\n","    self.flatten = Flatten()                      # out shape : (batch, 7*7*64)\n","    self.fc1 = MnistFullyConnected(7*7*64, 1024)  # out shape : (batch, 1024)\n","    self.dropout = Dropout(0.5)                   # out shape : unchanged\n","    self.fc2 = MnistFullyConnected(1024, 10)      # out shape : (batch, 10)\n","    self.softmax = Softmax()                      # out shape : unchanged\n","\n","  def call(self, x, training=False):\n","    x = tf.nn.relu(self.conv1(x))\n","    x = self.pool1(x)\n","    x = tf.nn.relu(self.conv2(x))\n","    x = self.pool2(x)\n","\n","    x = self.flatten(x)\n","    x = tf.nn.relu(self.fc1(x))\n","\n","    x = self.dropout(x, training=training) # behavior of dropout changes between train and test\n","    \n","    x = self.fc2(x)\n","    prob = self.softmax(x)\n","    \n","    return prob\n","\n","# Create an instance of the model\n","network = MnistDeepModel()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uGih-c2LgbJu"},"source":["Choose an optimizer and loss function for training: "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"u48C9WQ774n4","colab":{}},"source":["network_loss = tf.keras.losses.CategoricalCrossentropy()\n","\n","network_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ix4mEL65on-w"},"source":["Train the model using the same helper function we created for the perceptron"]},{"cell_type":"code","metadata":{"id":"3x-agRPccJ2k","colab_type":"code","outputId":"84473b71-b375-4e56-8802-e8add2bfc3fe","colab":{"base_uri":"https://localhost:8080/","height":191}},"source":["EPOCHS = 10\n","train_loop(EPOCHS, train_ds,  network, network_loss, network_optimizer)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1, Time 0:00:12.641701, Loss: 0.5832469463348389, Accuracy: 83.6816635131836\n","Epoch 2, Time 0:00:11.714608, Loss: 0.15075995028018951, Accuracy: 95.40833282470703\n","Epoch 3, Time 0:00:11.438412, Loss: 0.09863907843828201, Accuracy: 96.92666625976562\n","Epoch 4, Time 0:00:11.406362, Loss: 0.07229878753423691, Accuracy: 97.79166412353516\n","Epoch 5, Time 0:00:11.483109, Loss: 0.0608312226831913, Accuracy: 98.08833312988281\n","Epoch 6, Time 0:00:11.546565, Loss: 0.048298757523298264, Accuracy: 98.49832916259766\n","Epoch 7, Time 0:00:11.366544, Loss: 0.03957162797451019, Accuracy: 98.77333068847656\n","Epoch 8, Time 0:00:11.354394, Loss: 0.03575403615832329, Accuracy: 98.79500579833984\n","Epoch 9, Time 0:00:11.544217, Loss: 0.031041104346513748, Accuracy: 98.98666381835938\n","Epoch 10, Time 0:00:11.939246, Loss: 0.02749975211918354, Accuracy: 99.08333587646484\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IJKve4b3Jm1m","colab_type":"text"},"source":["Then perform evaluation over the trained network"]},{"cell_type":"code","metadata":{"id":"cKLYcSCOcKxU","colab_type":"code","outputId":"eee9f886-d0d1-47a3-d447-353f23d5fea6","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["test_loop(test_ds, network, network_loss)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Test Loss: 0.026255810633301735, Test Accuracy: 99.0999984741211\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"T4JfEh7kvx6m"},"source":["The image classifier is now trained to ~99% accuracy on this dataset. \n","By selecting a better set of hyperparameters \n","(number of epochs, batch size, number of neurons in the various  layers, number of layers, etc..)\n","we could obtain even better results. \n","To learn more, read the [TensorFlow tutorials](https://www.tensorflow.org/tutorials).\n","\n"]}]}